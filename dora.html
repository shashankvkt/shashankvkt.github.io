<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
    }
    
    h1 {
        font-size:32px;
        font-weight:300;
    }
    
    .disclaimerbox {
        background-color: #eee;     
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }
    
    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }
    
    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }
    
    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }
    
    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }
    
    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
        5px 5px 0 0px #fff, /* The second layer */
        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
        10px 10px 0 0px #fff, /* The third layer */
        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
        15px 15px 0 0px #fff, /* The fourth layer */
        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
        20px 20px 0 0px #fff, /* The fifth layer */
        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
        25px 25px 0 0px #fff, /* The fifth layer */
        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }

    .paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
        0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
        5px 5px 0 0px #fff, /* The second layer */
        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
        10px 10px 0 0px #fff, /* The third layer */
        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }
    
    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }
    
    hr
    {
        border: 0;
        height: 1px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }

    .equal-authors {
            text-align: right;
            margin-top: -20px;
            margin-right: 470px;
            font-size: 12px;
            color: #888; /* Adjust the color as needed */
        }
</style>

<html>


<body>
    <br>
    <center>
        <span style="font-size:36px">Is ImageNet worth 1 video? Learning strong image encoders from 1 long unlabelled video</span>
        <table align=center width=600px>
            <table align=center width=600px>
                <tr>
                    <td align=center width=100px>
                        <center>
                            <span style="font-size:24px"><a href="https://shashankvkt.github.io/">Shashanka Venkataramanan</a></span>
                        </center>
                    </td>
                    <td align=center width=100px>
                        <center>
                            <span style="font-size:24px"><a href="https://nayeemrizve.github.io/">Mamshad Rizve</a></span>
                        </center>
                    </td>
                    <td align=center width=100px>
                        <center>
                            <span style="font-size:24px"><a href="https://scholar.google.co.uk/citations?user=IUZ-7_cAAAAJ&hl=en/">Jo√£o Carreira</a></span>
                        </center>
                    </td>
                    <td align=center width=100px>
                        <center>
                            <span style="font-size:24px"><a href="https://yukimasano.github.io/">Yuki Asano*</a></span>
                        </center>
                    </td>
                    <td align=center width=100px>
                        <center>
                            <span style="font-size:24px"><a href="https://avrithis.net/">Yannis Avrithis*</a></span>
                        </center>

                    </td> 
                </tr>

            </table>
            <table align=center width=250px style="margin-bottom: 20px;">
                <tr>
                    <td align=center width=80px>
                        <center>
                            <span style="font-size:18px"><a href='https://arxiv.org/pdf/2310.08584.pdf'>[Paper]</a></span>
                        </center>
                    </td>
                    <td align=center width=80px>
                        <center>
                            <span style="font-size:18px"><a href='https://openreview.net/forum?id=Yen1lGns2o'>[OpenReview]</a></span>
                        </center>
                    </td>
                    <td align=center width=80px>
                        <center>
                            <span style="font-size:18px"><a href='https://huggingface.co/datasets/shawshankvkt/Walking_Tours'>[Dataset]</a></span><br>
                        </center>
                    </td>
                    <td align=center width=80px>
                        <center>
                            <span style="font-size:18px"><a href='https://github.com/shashankvkt/DoRA_ICLR24'>[Code]</a></span><br>
                        </center>
                    </td>
                    
                </tr>
            </table>
        </table>
    </center>

    <center>
       <p class="equal-authors" >(* equal last authors, order random)</p>
    </center>

    <hr>

    <table align=center width=850px>
        <center><h1>Abstract</h1></center>
        <tr>
            <td>
            <p>
                Self-supervised learning has unlocked the potential of scaling up pretraining to billions of images, since annotation is unnecessary. But are we making the best use of data? How more economical can we be? In this work, we attempt to answer this question by making two contributions. First, we investigate first-person videos and introduce a "Walking Tours" dataset. These videos are high-resolution, hours-long, captured in a single uninterrupted take, depicting a large number of objects and actions with natural scene transitions. They are unlabeled and uncurated, thus realistic for self-supervision and comparable with human learning.
            </p>

            <p>
                Second, we introduce a novel self-supervised image pretraining method tailored for learning from continuous videos. Existing methods typically adapt image-based pretraining approaches to incorporate more frames. Instead, we advocate a "tracking to learn to recognize" approach. Our method called DoRA leads to attention maps that DiscOver and tRAck objects over time in an end-to-end manner, using transformer cross-attention. We derive multiple views from the tracks and use them in a classical self-supervised distillation loss. Using our novel approach, a single Walking Tours video remarkably becomes a strong competitor to ImageNet for several image and video downstream tasks.
            </p>
        </td>

        </tr>
    </table>
    <br>

    <hr>
    <center><h1><a href='https://huggingface.co/datasets/shawshankvkt/Walking_Tours'>New Dataset: Walking Tour Videos</a></h1></center>
    <center>
    <table>
        <style>
        td {
            padding: 10px; /* Adjust the padding value as needed */
        }
    </style>
        <tr>
            <td>
                <img src="images/WT/1.gif" alt="GIF 1" width="300" height="200">
            </td>
            <td>
                <img src="images/WT/2.gif" alt="GIF 2" width="300" height="200">
            </td>
            <td>
                <img src="images/WT/3.gif" alt="GIF 3" width="300" height="200">
            </td>
        </tr>
        <tr>
            <td>
                <img src="images/WT/4.gif" alt="GIF 4" width="300" height="200">
            </td>
            <td>
                <img src="images/WT/5.gif" alt="GIF 5" width="300" height="200">
            </td>
            <td>
                <img src="images/WT/6.gif" alt="GIF 6" width="300" height="200">
            </td>
        </tr>
        <tr>
            <td>
                <img src="images/WT/7.gif" alt="GIF 7" width="300" height="200">
            </td>
            <td>
                <img src="images/WT/8.gif" alt="GIF 8" width="300" height="200">
            </td>
            <td>
                <img src="images/WT/9.gif" alt="GIF 9" width="300" height="200">
            </td>
        </tr>
    </table>
</center>

    <!-- <table align=center width=800px>
        <br>
        <tr>
            <center>
                <span style="font-size:28px"><a href=''>[Slides]</a>
                </span>
            </center>
        </tr>
    </table> -->
    <hr>
    <center><h1>DoRA: Attention-Based Multi-Object Tracking </h1></center>

    <table align=center width=420px>
        <center>
            <tr>
                <td>
                </td>
            </tr>
        </center>
    </table>
    <table align=center width=400px>
        <tr>
            <td align=center width=400px>
                <center>
                    <td><img class="round" style="width:950px" src="images/dora.jpg"/></td>
                </center>
            </td>
        </tr>
    </table>
    <table align=center width=850px>
        <center>
            <tr>
                <td>
                    We introduce DoRA, based on multi-object Discovery and Tracking. As shown in figure above, it leverages the attention from the [CLS] token of distinct heads in a vision transformer to identify and consistently track multiple objects within a given frame across temporal sequences. On these, a teacher-student distillation loss is then applied. Importantly, we do not use any off-the-shelf object tracker or optical flow network. This keeps our pipeline simple and does not require any additional data or training. It also ensures that the learned representation is robust.
                </td>
            </tr>
        </center>
    </table>


    <hr>
    <center><h1>Image and Video downstream tasks </h1></center>

    <center>
    <table>
        <tr>
            <td>
                <img src="images/im-vid-task.png" alt="Image 1" width="300" height="200">
                <p>Using just 1 video, DoRA outperforms DINO pretrained on ImageNet-1K on obj discovery and semantic segmentation.</p>
            </td>
            <td>
                <img src="images/pretrain.png" alt="Image 2" width="300" height="200">
                <p>Finetuning on different image-based datasets. DoRA pretrained on 10 WTour videos outperforms DINO pretrained on ImageNet-1K</p>
            </td>
            <td>
                <img src="images/diff-vids.png" alt="Image 3" width="300" height="200">
                <p>Linear Probing on different video pretraining datasets e.g. Epic-Kitchens, Kinetics-400 and Movies (randomly chosen from romantic genre)</p>
            </td>
        </tr>
        
    </table>
</center>


<hr>
    <center><h1>Emergent Multi-Object Masks</h1></center>
    <center>
    <table>
        <style>
        td {
            padding: 10px; /* Adjust the padding value as needed */
        }
    </style>
        <tr>
            <td>
                <img src="images/WT/WT-1.gif" alt="GIF 1" width="300" height="200">
            </td>
            <td>
                <img src="images/WT/WT-1-mask.gif" alt="GIF 2" width="300" height="200">
            </td>
        </tr>
        <tr>
            <td>
                <img src="images/WT/WT-2.gif" alt="GIF 4" width="300" height="200">
            </td>
            <td>
                <img src="images/WT/WT-2-mask.gif" alt="GIF 5" width="300" height="200">
            </td>
        </tr>
    </table>
</center>
    
    <br>
    <hr>
    <table align=center width=450px>
        <center><h1>Paper and Supplementary Material</h1></center>
        <tr>
            <td><a href=""><img class="layered-paper-big" style="height:175px" src="images/paper.png"/></a></td>
            <td><span style="font-size:14pt">S. Venkataramanan, M. Rizve, J. Carreira, Y.M. Asano, Y. Avrithis<br>
                <b>Is ImageNet worth 1 video? Learning strong image encoders from 1 long unlabelled video</b><br>
                In International Conference on Learning Representations, 2024.<br>
                (hosted on <a href="">ArXiv</a>)<br>
                <!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
                <span style="font-size:4pt"><a href="https://arxiv.org/pdf/2310.08584.pdf"><br></a>
                </span>
            </td>
        </tr>
    </table>
    <br>

    <table align=center width=600px>
        <tr>
            <td><span style="font-size:14pt"><center>
                <a href="data/dora_2023.bib">[Bibtex]</a>
            </center></td>
        </tr>
    </table>

    <hr>
    <br>

    

<br>
</body>
</html>