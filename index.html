<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shashanka Venkataramanan</title>
  
  <meta name="author" content="Shashanka Venkataramanan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
<link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <link rel   = "stylesheet" href    ="https://use.fontawesome.com/releases/v5.0.7/css/all.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shashanka Venkataramanan</name>
              </p>
              <p> I am a Doctoral student with the LinkMedia team at  <a href="https://www.inria.fr/en" target="blank"> INRIA Rennes</a>, where I am advised by some amazing researchers  <a href="https://avrithis.net/" target="blank">Dr. Yannis Avrithis</a>, <a href="http://people.irisa.fr/Ewa.Kijak/" target="blank">Dr. Ewa Kijak</a> and <a href="http://people.rennes.inria.fr/Laurent.Amsaleg/" target="blank">Dr. Laurent Amsaleg</a>. My work mainly focuses on developing algorithms to enhance instance and category-level visual representations in the supervised and self-supervised settings. <br><br>


              I recently graduated with my Masters in C.S from the <a href="https://www.crcv.ucf.edu/" target="blank"> Center for Research in Computer Vision </a> at the University of Central Florida, USA with <a href="https://www.crcv.ucf.edu/person/abhijit-mahalanobis/" target="blank"> Dr. Abhijit Mahalanobis</a>. Prior joining UCF, I spent some time at the beautiful <a href="https://www.iisc.ac.in/" target="blank"> Indian Institute of Science</a> as a research assistant with <a href="http://www.ee.iisc.ac.in/people/faculty/soma.biswas/" target="blank"> Dr. Soma Biswas </a> and <a href="https://sites.google.com/site/sivaramprasad443/" target="blank"> Dr. Sivaram Prasad Mudunuri</a>. I feel lucky and am grateful to have worked with them. </p> 

              <p style="text-align:center">
                <a href="mailto:shawshankv16@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/cv_shashank.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=CbfH47IAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/shawshank_v">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/shashank-venkataramanan-1b2b9993/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/shashankvkt">GitHub</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/shashank_profile.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/shashank_profile.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
  
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> July 2023: Attending the <a href="https://iplab.dmi.unict.it/icvss2023/Home" target="_blank">ICVSS</a> Summer School in Sicily <br>

                &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> January 2023: <a href="https://arxiv.org/pdf/2301.02240.pdf" target="_blank">SkipAT</a>  released on arXiv <br>
                &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> September 2022: Attending the <a href="https://ivi.fnwi.uva.nl/ellis/event/video-understanding-symposium-2022-8-9-september-2022-amsterdam/" target="_blank"> ELLIS Video Sympsium</a> (invite only) in Amsterdam <br>
                &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> June 2022: Starting internship with Video Efficiency team at <a href="https://www.qualcomm.com/research/artificial-intelligence/ai-research" target="_blank">Qualcomm AI Research </a> in Amsterdam <br>
                &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> May 2022: <a href="https://arxiv.org/pdf/2206.14868.pdf" target="_blank">MultiMix</a> released on arXiv <br>
                &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> February 2022: <a href="https://arxiv.org/pdf/2103.15375.pdf" target="_blank">AlignMixup</a> is accepted to CVPR 2022<br>
                &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> January 2022: <a href="https://arxiv.org/pdf/2103.15375.pdf" target="_blank">Metrix</a> is accepted to ICLR 2022<br>
                &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> December 2021: Teaching Deep Metric learning course at MathSTIC <br>
                &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> September 2021: Attending the <a href="https://ellis.eu/events/ellis-doctoral-symposium" target="_blank">ELLIS doctoral symposium</a> in Tubingen <br>
                &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> December 2020: Started my PhD at INRIA<br>
                
              </p>
            </td>
          </tr>
        </tbody></table>
        
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
             <tr>
             <td style="padding:20px;width:100%;vertical-align:middle"> 
               <heading>Publications</heading>
            </td>
        </tr> 
    </tbody></table>  

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    
  <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/skipat_2023.png" alt="skipAT-2023" height="60" width="180" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2301.02240.pdf" id="MCG_journal">
                <papertitle>Skip-Attention: Improving Vision Transformers by Paying Less Attention</papertitle>
              </a>
              <br>
              <strong>Shashanka Venkataramanan</strong>, <a href="https://scholar.google.be/citations?user=h1IvkAsAAAAJ&hl=en">Amir Ghodrati</a>, <a href="https://yukimasano.github.io/">Yuki M. Asano</a>, <a href="https://www.porikli.com/">Fatih Porikli</a>, <a href="https://habibian.github.io/">Amirhossein Habibian</a>
              <br>
              <em>arXiv</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2301.02240.pdf">paper</a>/
              <a href="data/skipat_2023.bib">bibtex</a> 
              <p>
       <!-- PivoTAL approaches Weakly-Supervised Temporal Action Localization from localization-by-localization perspective by learning to localize the action snippets directly. To this end, PivoTAL introduces a novel algorithm that exploits the inherent spatiotemporal structure of the video data in the form of action-specific scene prior, action snippet generation prior, and learnable Gaussian prior to generate pseudo-action snippets. These pseudo-action snippets provide additional supervision, complementing the weak video-level annotations during training. -->
    </p>
            </td>
          </tr>
  <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/multimix_2022.png" alt="MultiMix-2023" height="80" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2206.14868.pdf" id="MCG_journal">
                <papertitle>Teach me how to Interpolate a Myriad of Embeddings</papertitle>
              </a>
              <br>
              <strong>Shashanka Venkataramanan</strong>, <a href="http://people.irisa.fr/Ewa.Kijak/">Ewa Kijak</a>, <a href="http://people.rennes.inria.fr/Laurent.Amsaleg/">Laurent Amsaleg</a>, <a href="https://avrithis.net/">Yannis Avrithis</a>
              <br>
              <em>arXiv</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2206.14868.pdf">arxiv</a> /
              <a href="data/multimix_2022.bib">bibtex</a> 
              <!-- <a href="https://github.com/DAVEISHAN/TimeBalance">code</a> -->
              <p>
       <!-- We propose a student-teacher semi-supervised learning framework, where we distill knowledge from a temporally-invariant and temporally-distinctive teacher. Depending on the nature of the unlabeled video, we dynamically combine the knowledge of these two teachers based on a novel temporal similarity-based reweighting scheme. -->
    </p>
            </td>
          </tr>
          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/alignmixup_cvpr2022.jpg" alt="alignmix-cvpr2022" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2103.15375.pdf" id="MCG_journal">
                <papertitle> AlignMixup: Improving Representations By Interpolating Aligned Features</papertitle>
              </a>
              <br>
              <strong>Shashanka Venkataramanan</strong>, <a href="http://people.irisa.fr/Ewa.Kijak/">Ewa Kijak</a>, <a href="http://people.rennes.inria.fr/Laurent.Amsaleg/">Laurent Amsaleg</a>, <a href="https://avrithis.net/">Yannis Avrithis</a>
              <br>
              <em>CVPR</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2103.15375.pdf">arxiv</a> /
              <a href="https://inria.hal.science/hal-03620779/file/AlignMix_CVPR22_cameraReady.pdf">HAL</a> /
              <a href="data/alignmixup_cvpr2022.bib">bibtex</a> /
              <a href="https://github.com/shashankvkt/AlignMixup_CVPR22">code</a>
        <p></p>
              <p>
    <!-- We propose a new method for open-world semi-supervised learning that utilizes sample uncertainty and incorporates prior knowledge about class distribution to generate reliable class-distribution-aware pseudo-labels for samples belonging to both known and unknown classes. -->
        </p>
        
            </td>
          </tr>
    
    
          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/metrix_iclr2022.png" alt="metrix-iclr2022" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2106.04990.pdf" id="MCG_journal">
                <papertitle>It Takes Two to Tango: Mixup for Deep Metric Learning</papertitle>
              </a>
              <br>
              <strong>Shashanka Venkataramanan</strong>, <a href="http://users.ntua.gr/psomasbill/">Bill Psomas</a>, <a href="http://people.irisa.fr/Ewa.Kijak/">Ewa Kijak</a>, <a href="http://people.rennes.inria.fr/Laurent.Amsaleg/">Laurent Amsaleg</a>, <a href="http://users.ntua.gr/karank/">Konstantinos Karantzalos</a>, <a href="https://avrithis.net/">Yannis Avrithis</a>
              <br>
              <em>ICLR</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2106.04990.pdf">arxiv</a> /
              <a href="data/metrix_iclr2022.bib">bibtex</a> /
              <a href="https://github.com/billpsomas/Metrix_ICLR22">code</a> /
              <a href="https://iclr.cc/virtual/2022/poster/6337">video</a> /
              <a href="https://github.com/billpsomas/Metrix_ICLR22/blob/master/.github/slides.pdf">slides</a> /
              <a href="https://github.com/billpsomas/Metrix_ICLR22/blob/master/.github/poster.pdf">poster</a>
              <p>
        <!-- OpenLDN utilizes a pairwise similarity loss with bi-level optimization to discover novel classes and transforms the open-world SSL problem into a standard SSL problem, outperforming current state-of-the-art methods with better accuracy/training time trade-off. -->
    </p>
            </td>
          </tr>
    
    
          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/cavga_eccv2020.png" alt="cavga-eccv2020" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1911.08616" id="MCG_journal">
                <papertitle> Attention Guided Anomaly Localization in Images</papertitle>
              </a>
              <br>
               <strong>  Shashanka Venkataramanan</strong>, <a href="https://www.merl.com/people/kpeng">Kuan-Chuan Peng</a>, <a href="http://rajatvikramsingh.github.io/">Rajat Vikram Singh</a>, <a href="https://www.crcv.ucf.edu/person/abhijit-mahalanobis/">Abhijit Mahalanobis </a>
              <br>
              <em>ECCV</em>, 2020
              <br>
              <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123620477.pdf">paper</a> / 
              <a href="data/cavga_eccv2020.bib">bibtex</a> /
              <a href="https://arxiv.org/abs/1911.08616">arxiv</a> / 
              <a href="https://youtu.be/b-EQr-fGPWo"> teaser video</a> / 
              <a href="https://www.youtube.com/watch?v=n0t4gthI0fM"> main video</a> 
              <p>
    <!-- UNICON is a robust sample selection approach for training with high label noise. It incorporates a Jensen-Shannon divergence based uniform sample selection mechanism and contrastive learning. -->
  </p>
            </td>
          </tr>
          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/tclr_icip2020.jpg" alt="tclr-icip2020" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2101.07974" id="MCG_journal">
                <papertitle>Target Detection in Cluttered Environments using Infra-red Images</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/bwmcintosh/">Bruce McIntosh</a>, <strong>  Shashanka Venkataramanan</strong>,  <a href="https://www.crcv.ucf.edu/person/abhijit-mahalanobis/">Abhijit Mahalanobis </a>
              <br>
              <em>ICIP</em>, 2020
              <br>
              <a href="https://www.crcv.ucf.edu/wp-content/uploads/2020/07/icip2020_photoready.pdf">arxiv</a> / 
              <a href="data/tclr_icip2020.bib">bibtex</a> 
              
              <p>
              <!-- We propose a new temporal contrastive learning framework for self-supervised video representation learning, consisting of two novel losses that aim to increase the temporal diversity of learned features. The framework achieves state-of-the-art results on various downstream video understanding tasks, including significant improvement in fine-grained action classification for visually similar classes.</p> -->
            </td>
          </tr>
          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/dalign_tifs2019.jpg" alt="dalign-tifs2019" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2103.01315" id="MCG_journal">
                <papertitle>Dictionary alignment with re-ranking for low-resolution NIR-VIS face recognition</papertitle>
              </a>
              <br>
              <a href="https://sites.google.com/site/sivaramprasad443/">Sivaram Prasad Mudunuri*</a>, <strong>  Shashanka Venkataramanan*</strong>,  <a href="http://www.ee.iisc.ac.in/people/faculty/soma.biswas/">Soma Biswas </a>
              <br>
              <em>IEEE Transactions on Information Forensics and Security</em>, 2019
              <br>
              <a href="http://www.ee.iisc.ac.in/people/faculty/soma.biswas/Papers/DAlign_tifs2019.pdf">paper</a> / 
              <a href="data/dalign_tifs2019.bib">bibtex</a> /
              <a href="https://github.com/shashankvkt/dualranking_TIFS_2018">code</a> 
              <p>
        <!-- We propose a novel training mechanism for few-shot learning that simultaneously enforces equivariance and invariance to geometric transformations, allowing the model to learn features that generalize well to novel classes with few samples.  -->
        </p>
            </td>
          </tr>

         

        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
             <tr>
             <td style="padding:20px;width:100%;vertical-align:middle"> 
               <heading>Academic Services</heading>
            </td>
        </tr> 
    </tbody></table>  

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <p>
            <li><span> [2023] Reviewer for <a href="https://cvpr2023.thecvf.com/" target="_blank">CVPR</a>
            <li><span> [2022] Reviewer for <a href="https://iclr.cc/" target="_blank">ICLR</a>, <a href="https://cvpr2022.thecvf.com/" target="_blank">CVPR</a>, <a href="https://eccv2022.ecva.net/" target="_blank">ECCV</a>, <a href="https://nips.cc/Conferences/2022/" target="_blank">NeurIPS</a> </span></li>
            <li><span> [2021] Reviewer for <a href="http://iccv2021.thecvf.com/home" target="_blank">ICCV</a>, <a href="https://2021.acmmm.org/" target="_blank">ACM Multimedia</a>, <a href="https://www.acmmmasia.org/" target="_blank">ACM Multimedia Asia</a> </span></li>

              </p>
            </td>
          </tr>

  </tbody></table>      
        

  </table>
</body>

</html>
